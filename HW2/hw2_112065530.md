# Homework 2

# 1. Title, name, student ID

Homework 2 Report, 張博皓, 112065530

# 2. Implementation

## Program A

### 程式運行大綱

- **核心架構**

首先將參數輸入以結構（<code>mandelbrot_params_t</code>）儲存，讓所有的 Threads 都可以取用此共享的參數資訊。

結構 <code>mandelbrot_params_t</code> 包含：輸入參數（<code>iters</code>、<code>left</code>、<code>right</code>、...、<code>image</code>等），其中 <code>image</code> 為所有 Thread，要根據自己的任務範圍，將運算的結果填入到此二維陣列中。

> image = malloc(width * height * sizeof(int))

---

建立 Thread Pool， Thread 數量為 CPU 的核心數 <code>CPU_COUNT(&cpu_set)</code>。

主程序首先分配空間來存放 Thread Pool 的結構（<code>thread_pool_t</code>），這個結構包含了管理 Threads 和 Task 所需的所有必要資訊（Task Queue、<code>task_capacity</code>、<code>task_count</code>、<code>queue_mutex</code>、...等）。

Thread Pool 中有一個 Task Queue 用於存放需要被 Thread 計算的任務，Thread 可以不斷地從 Task Queue 中取用任務直到所有任務都被計算完成。

為了確保 Thread safety，當 Thread 要取用任務時，會使用 <code>queue_mutex</code> 的互斥鎖停止其他 Thread 取用 Task Queue，防止同一個任務同時被多次取用。

Task Queue 有存放上限，Thread 可以透過 <code>queue_not_empty</code>、<code>queue_not_full</code> 等條件變數檢查 Task Queue 是否有無任務以及主程序是否可以新增任務，否則的話就必須等待。

---

Thread Pool 的方法相比於平均分配任務給予 Thread 更具有時間效益。

原因在於平均分配任務會有 Load Unbalance 的問題，由於計算 Mandelbrot Set 圖像中的每一個區塊的工作量並不相同，有些區塊需要較久的運算時間，有些區塊則否，因此在平均分配任務的情況下，有些 Thread 會因為被分配到較少工作量的區塊提前結束而產生閒置時間。

Thread Pool 以不斷分配任務的方式給予各個 Thread 工作，直到所以任務皆被計算完成，因在計算較困難任務的同時可以完成更多簡單的任務，如此可以增加 Thread 的利用效率，減少閒置時間的產生。

- **分配及計算**

切分輸出圖像的高度，以每一行（Row）圖像為任務單位，計算每一行圖像中每一個像素的 <code>repeats</code> 值，主程序按照由上到下的順序一行一行將任務新增至 Thread Pool，由各個 Thread 進行計算。

當 Thread 從 Task Queue 中取得任務後，得到此任務需要被計算的圖像行位置，根據行位置計算每個像素的 <code>repeats</code> 值。

在計算圖像行的 <code>repeats</code> 值時，由於課堂中的機器支援 AVX 指令集，因此採用 SIMD 的方式進行向量化的計算處理，使用 AVX-512 一次計算 32 個像素點的 <code>repeats</code> 值，分成 4 組，每組用一個向量暫存器處理 8 個像素點，並且使用 <code>alignas(64)</code> 確保記憶體對齊，向量化的計算處理大幅提升了計算效能。

- **結束並輸出**

在所有任務皆計算完成後，進行清理和關閉 Thread Pool，將 <code>image</code> 中的 <code>repeats</code> 值轉換並儲存為 PNG 圖片檔案

## Program B

### 程式運行大綱

- **核心架構**

程式混和 MPI 以及 OpenMP，並使用 <code>omp_set_num_threads()</code> 設定 OpenMP 可以建立的 Thread 數，將 Thread 數設定為 CPU 的核心數 <code>CPU_COUNT(&cpu_set)</code>。

程式需要做兩次的計算任務分配，首先第一次於 MPI 層級，MPI 層級需要分配計算任務範圍給予各個 Process，計算任務範圍會是以切分圖像的高度為單位，每個 Process 會被分配到多行圖像的計算任務；分配好 MPI 層級後則是 OpenMP 層級，OpenMP 層級需要切分 Process 中被分配到的多行圖像計算任務為單位，以每一行圖像為計算任務進行計算。

- **分配及計算**

在 MPI 層級分配計算任務時，採用循環分配（Round-robin）方式，每個 Process 負責相隔 <code>size</code>（總Process數） 距離的圖像行，例如：若有 4 個 Process，rank 0 處理第 0, 4, 8... 列，rank 1 處理第 1, 5, 9... 列，依此類推。以循環分配的方式，可以提升計算效率，原因在於圖像中各個區塊的工作量並不相同，循環分配可以輕微改善 Load Unbalance 的問題。

OpenMP 層級以一圖像行為任務單位，使用 OpenMP 動態排程（Dynamic schedule）來分配工作，有助於負載平衡。動態排程會將 for 迴圈的所有 Iteration 依序以指定 <code>chunk_size</code> 做切割成數個 Chunk。當 Thread 執行完一個 Chunk 後，他會在去找別的 Chunk 來執行，類似於 Program A 的 Task Queue。

在計算圖像行的 <code>repeats</code> 值時，同 Program A 採用 AVX-512 指令集加速圖像行的計算處理。

- **結束並輸出**

每個 Process 計算完的結果必須由 rank 0 負責收集，並寫入 PNG 檔案，其他 Process 完成計算後將結果傳送給 rank 0。為避免因其他 Process 傳送計算結果的時機不同導致寫入 PNG 時順序錯亂的問題，rank 0 先建立一個暫時的 Rows Buffer 將其他 Process 的計算結果依照行位置填入後，再依序掃描 Rows Buffer 中的資料建立 PNG 圖像

# 3. Experiment & Analysis

### (a). System Spec

我的實驗都在課堂提供的 QCT Cluster 中完成。

### (b). Performance Metrics

- Program A

我使用<code>C++11 <chrono></code>函式庫做為時間計算的測量工具，<code>high_resolution_clock::now()</code>可以回傳一個 Time Point 代表現在的時間，我可以透過 Time Point 相差來測量目標執行的所需時間。

我主要測程式一開始執行到準備結束前作為<code>Total_time</code>。

測量將所有以圖像行為任務單位的任務，分配且計算完成的經過時間作為<code>Task_time</code>。

```C++
auto TASK_t_s = std::chrono::high_resolution_clock::now();

for (int row = 0; row < shared_params.height; row++) {
    thread_pool_add_task(pool, row, row + 1);
}

auto TASK_t_e = std::chrono::high_resolution_clock::now();
```

測量<code>write_png()</code>的時間總和作為<code>IO_time</code>。

將<code>Total_time</code>扣除<code>IO_time</code>、<code>Task_time</code>後，其餘的時間做為<code>CPU_time</code>。

- Program B

我使用<code>MPI_Wtime()</code>作為我的測量工具，<code>MPI_Wtime</code>會回傳過去任意時間點到當前的雙精度秒數，我可以透過時間相差來測量目標執行的所需時間。

我主要測量<code>MPI_Init()</code>前到<code>MPI_Finalize()</code>作為<code>Total_time</code>。

測量<code>png_write_row()</code>的時間總和作為<code>IO_time</code>。

測量 rank 0 在執行 <code>MPI_Recv</code> 的時間總和作為<code>Comm_time</code>。

測量計算每一行的任務作為<code>Task_time</code>。

將<code>Total_time</code>扣除<code>IO_time</code>、<code>Comm_time</code>、<code>Task_time</code>後，其餘的時間做為<code>CPU_time</code>。

---

![outa1](https://hackmd.io/_uploads/H17N65V-Jg.png)


我使用課堂中提供的"strict34.txt"與"fast07.txt"作為我實驗的測資。將 strict34 作為各實驗的主要運行對象，是因為 strict34 有較大的圖片以及 Mandelbrot set 範圍需要被計算，可以較明顯的看出程式碼的平行程度。另外使用 fast07 作為無向量化與有向量化進行 <code>repeats</code> 值計算的實驗對象就可以明顯比較出差異，否則無向量化程式計算速度可能過慢造成無法測量。我將每次實驗做 3 次減少實驗中產生的隨機誤差。

## ii. Plots: Speedup Factor & Profile

### Experiment 1

>**使用單一 Process 數（n=1），並且逐步增加 Thread 數，觀察不同 Thread 數下程式性能的表現。**

#### Program A

- 表現數據簡表

| Process Num | Task Time | I/O Time | CPU Time | Total Time |
| -------- | -------- | -------- | -------- | -------- |
| 1 | 40.51 | 1.54 | 0.15 | 42.20 |
| 2 | 20.18 | 1.54 | 0.15 | 21.87 |
| ... | ... | ... | ... | ... |
| 12 | 3.24 | 1.53 |0.16 | 4.93 |

![image1](https://hackmd.io/_uploads/SJTSa94-yg.png)
- 固定1個 Process 數，逐步增加 Thread 數的時間分配圖。RUN TIME 單位為秒（s）

![image2](https://hackmd.io/_uploads/HJbd65Ebkx.png)
- 固定1個 Process 數，逐步增加 Thread 數的時間比例圖。RUN TIME 單位為秒（s）

![image3](https://hackmd.io/_uploads/S1O_T9Vb1l.png)
- 固定1個 Process 數，逐步增加 Thread 數的 Speedup Factor，圖中藍色線為理想值、橘色線為實際值。

#### Program B

- 表現數據簡表

| Process Num | Task Time | I/O Time | CPU Time | Total Time |
| -------- | -------- | -------- | -------- | -------- |
| 1 | 40.57 | 1.41 | 0.39 | 42.37 |
| 2 | 20.28 | 1.41 | 0.30 | 21.98 |
| ... | ... | ... | ... | ... |
| 12 | 3.39 | 1.41 | 0.18 | 4.97 |

![image4](https://hackmd.io/_uploads/rkouAc4-Jg.png)
- 固定1個 Process 數，逐步增加 Thread 數的時間分配圖。RUN TIME 單位為秒（s）

![image5](https://hackmd.io/_uploads/BkJFA54bJl.png)
- 固定1個 Process 數，逐步增加 Thread 數的時間比例圖。RUN TIME 單位為秒（s）

![image6](https://hackmd.io/_uploads/HJmFAqEbkl.png)
- 固定1個 Process 數，逐步增加 Thread 數的 Speedup Factor，圖中藍色線為理想值、橘色線為實際值。

### Observation

從這些圖表的觀察中，可以看到一些有趣的現象。首先在執行時間的表現上，Program A 和 B 都有相似的模式：當我們從單一 Thread 數開始增加 Thread 數量時，整體執行時間呈現明顯的下降趨勢。特別是在單一 Thread 的情況下，兩個程式都需要大約40至42個單位的執行時間。

在資源使用的分配上，隨著 Thread 數量的增加，我們注意到一個顯著的變化：I/O 操作在整體執行時間中所占的比例逐漸上升，而 Task 所需的時間比例則相應下降。CPU 的使用量（除了 Task 與 I/O 之外剩餘的時間）在整個過程中維持相對較低的水準。這個趨勢在 Thread 數達到9到12時特別明顯，此時 I/O 操作的比重已經明顯高於較少 Thread 數時的情況。

其實，在圖表中我們只需要觀察 Task 所需的時間，因為 I/O 只會在主程序做輸出，因此 I/O 時間並不會隨著 Thread 數增加而有所變化。

觀察加速比的表現，我們可以看到理想加速比和實際加速比之間存在著漸進的差距。當 Thread 數較少（特別是在4個以下）時，實際加速比相當接近理想情況。然而，隨著 Thread 數量增加，這個差距逐漸擴大。到達12個 Thread 時，雖然理想加速比達到12，但實際加速比只有大約8.5左右。透過這個加速比圖表我領悟到以 Thread 為主的平行化相比於以 Process 為主的平行化具有更好的加速比。

使用 Pthread 與 OpenMP 版本的兩支程式有非常相似效能表現，我認為原因是我在 Pthread 實作的 Thread Pool 與 OpenMP 中 Dynamic scheduling 意義上相同，都是透過分發任務給予各個 Thread 進行處理，因此整體程式除了程式碼之外，在單一 Process 數下的效能上具有差不多的表現（但 Program B 可支援更多 Process 數進行處理）。

### Experiment 2

>**Program B 在固定 Thread 數（c=2、4），並且逐步增加 Process 數，觀察不同 Process 數下程式性能的表現。**

- 固定2個 Thread 數，表現數據簡表

| Process Num | Task Time | I/O Time | Comm Time |CPU Time | Total Time |
| -------- | -------- | -------- | -------- | -------- | -------- |
| 1 | 20.28 | 1.41 | 0.00 | 0.25 | 21.94 |
| 2 | 10.14 | 1.41 | 0.02 | 0.26 | 11.83 |
| ... | ... | ... | ... | ... | ... |
| 12 | 1.74 | 1.41 | 0.01 | 0.58 | 3.74 |

![image7](https://hackmd.io/_uploads/Byr4OsEbyx.png)
- 固定2個 Thread 數，逐步增加 Process 數的時間分配圖。RUN TIME 單位為秒（s）

![image8](https://hackmd.io/_uploads/Bk9VdoVWJg.png)
- 固定2個 Thread 數，逐步增加 Process 數的 Speedup Factor，圖中藍色線為理想值、橘色線為實際值。

---

- 固定4個 Thread 數，表現數據簡表

| Process Num | Task Time | I/O Time | Comm Time |CPU Time | Total Time |
| -------- | -------- | -------- | -------- | -------- | -------- |
| 1 | 10.14 | 1.41 | 0.00 | 0.21 | 11.76 |
| 2 | 5.07 | 1.41 | 0.01 | 0.25 | 6.75 |
| ... | ... | ... | ... | ... | ... |
| 12 | 1.10 | 1.42 | 0.02 | 0.52 | 3.05 |

![image9](https://hackmd.io/_uploads/Bypj_oVZye.png)
- 固定4個 Thread 數，逐步增加 Process 數的時間分配圖。RUN TIME 單位為秒（s）

![image10](https://hackmd.io/_uploads/HkxhOs4Zke.png)
- 固定4個 Thread 數，逐步增加 Process 數的 Speedup Factor，圖中藍色線為理想值、橘色線為實際值。

### Observation

在觀察以上圖表時。首先，從執行時間的角度來看，無論是在 C=2 還是 C=4 下，隨著 Process 數增加，整體執行時間都呈現明顯的下降趨勢。特別值得注意的是，C=4 初始執行時間約為 12 秒，相較於 C=2 的 22 秒明顯要短得多。不過，兩者都表現出一個共同特點：當 Process 數超過8個後，執行時間的減少趨勢變得相當緩慢。

儘管 Task 時間仍然是最主要的組成部分，但我們可以觀察到，隨著 Process 數增加，Comm 和 I/O 所佔的比例逐漸上升。而 CPU 使用時間雖然在整體中佔比較小，但始終維持著一定的存在。

從加速比的表現來看，兩者呈現出明顯的差異。C=2 能夠達到約6倍的實際加速比，而 C=4 的最高實際加速比僅約4倍。然而，兩者都存在一個共同的現象：隨著 Process 數增加，理想加速比與實際加速比之間的差距逐漸擴大。特別是在 C=4 時，當 Process 數超過6個後，實際加速比的增長趨勢變得相當平緩。

比較兩者，我們可以發現一些有趣的對比。雖然 C=4 在整體執行時間上表現較好，但其加速比的效果反而不如 C=2 理想。值得注意的是，兩者的理想加速比曲線幾乎完全相同，而且在各個時間組成部分的比例分布上也相當類似，主要的差異在於 C=4 的絕對數值普遍較小。

比較Experiment 1，可以發現更多 Process 數來處理，確實有明顯的效能提升。

### Experiment 3

>**觀察現有向量化的程式與與向量化的程式的性能表現差異。**

![image11](https://hackmd.io/_uploads/BkmuosEbkg.png)
- 固定1個 Process 數，逐步增加 Thread 數的時間分配圖。RUN TIME 單位為秒（s）

### Observation

從這張時間分配圖中，我們可以觀察到一些值得注意的現象。首先，在整體執行時間的表現上，未經向量化處理的版本明顯需要較長的執行時間。這點特別反映在單一 Thread 的情況下，未向量化版本需要約1.4秒，而經過向量化處理的版本僅需約0.2秒。雖然兩種版本的執行時間都會隨著 Thread 數量的增加而減少，但它們的改善幅度有著明顯的差異。

從時間降低的趨勢來看，未向量化版本在執行緒數從1增加到4的過程中展現出最顯著的改善效果，而向量化版本則呈現較為平緩的下降趨勢。有趣的是，當執行緒數超過8時，這兩個版本的執行時間改善效果都變得相當有限。

## iii. Discussion

> ### Compare and discuss the scalabillity and load balance of your implementaions.

以第一個實驗來說，主要的執行時間都被 Task 給佔據，可知執行 Task 是程式最大的瓶頸。使用更多 Thread 來執行這些任務，雖然可以壓縮 Task 的執行時間，但是發現到某程度的 Thread 數量，壓縮時間的程度就會趨於減緩。這是因為現在切分任務的方式是使用圖像行為單位進行切分，處理圖像行的程式碼雖然已經經過向量化處理，但對於面對複雜的計算的時候，還是需要花費一些基礎的計算時間，這些基礎時間不會隨著 Thread 增加而減少。在更早以前的實作中，有嘗試使用以 1 個圖像像素作為任務單位進行計算，但整體時間相較於以圖像行為單位還有來的久，我認為原因在於，切分更細緻顆粒度的任務會讓 Thread 在取用 Task Queue 時因為 Mutex Lock 時讓其他 Thread 造成等待，在顆粒度更小的情況下等待時間會累積形成瓶頸，雖然顆粒度細可能可以解決基礎時間的問題，但整理瓶頸還是沒有圖像行來的有效益。

---

再來討論第二個實驗，比較第一個實驗可以發現，從MPI層級再分配任務給各個 Process 有顯著的效能提升，再搭配OpenMP就可以有更好的表現。在MPI層級的任務是以循環（Round-robin）的方式進行分配，比較我在早期是以連續區域分配更具有負載平衡效益。不知道實際原因，我發現 rank 0 在等待接收資料的時間幾乎可以忽略，我推測是我在等待接收資料之前的時間，有些將 rank 0 本身的資料進行 <code>repeats</code> 值的顏色計算，這段時間足以讓其他 rank 進行資料傳輸，所以 rank 0 幾乎不用等待其他 rank 的傳輸時間。一樣，Program B 與 Program A 具有相似的效益，程式碼上差別只在於使用 Pthread 與 OpenMP 的方式，但實作方式類似（我發現我自己實作的　Pthread　版本比　OpenMP　版本有些微較快的表現），因此具有差不多的性能表現，但 Program B 可以由多個 Process 一起合作完成，執行效率更好。

# 4. Experiences / Conclusion

在探索平行計算的奧妙過程中，我深刻體會到任務分割的藝術在提升執行效率時扮演著舉足輕重的角色。這就像是將一幅巨大的畫作切割成恰到好處的碎片，每個碎片都需要精心考量，既要保持其完整性，又要確保能夠順暢地重組。

在任務分割的過程中，我們需要全方位地考量各種可能的效能瓶頸，如 I/O 操作、記憶體存取、CPU調度等因素。這讓我明白，任務切分並非越細緻越理想，過度細分反而可能導致任務之間的等待時間累積，猶如過度切碎的拼圖反而增加了重組的複雜度。

更令人驚嘆的是，現代平行計算架構為我們提供了多層次的任務分割策略：

- Process 層級：適合處理獨立性強、需要隔離的大型任務
- Thread 層級：善於處理共享資源、通訊頻繁的中等任務
- Vectorize 層級：特別適合處理規律的數值運算和資料密集型操作

通過掌握這種層層遞進的任務分割方法，我們能夠像精密的工匠一樣，根據問題的特性選擇最適合的顆粒度，讓程式在運行時展現出最佳的性能表現。這種多層次的優化策略，不僅提升了程式的執行效率，更開啟了我對現代平行計算架構深層次的理解之門。