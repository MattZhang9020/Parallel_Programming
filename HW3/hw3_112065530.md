# Homework 3

# 1. Title, name, student ID

Homework 3 Report, 張博皓, 112065530

# 2. Implementation

## a. Which algorithm do you choose in hw3-1?

在 hw3-1 我選擇與 Spec 提供的演算法相同，我採用 Blocked Floyd-Warshall Algorithm。

> e. Briefly describe your implementations in diagrams, figures or sentences.

![image](https://hackmd.io/_uploads/r1qkbB24ke.png)

與 `seq.cc` 的做法大致相同，首先計算需要做多少次 `round`：

```cpp
round = (n + B - 1) / B; // 意思是 n / B 取 Ceiling
```

我在 hw3-1 設定 `B` 為 64，意思是將 Dist 切分成數個最多由 64 * 64 個元素組成的 Block。

然後分別計算：
- Phase 1：計算 Pivot Block 的 Floyd-Warshall Algorithm。
- Phase 2：計算 Pivot Row / Pivot Column Blocks 的 Floyd-Warshall Algorithm。
- Phase 3：計算剩餘 Blocks 的 Floyd-Warshall Algorithm。

最後將計算完成的 Dist 簡單輸出到輸出檔案中。

---

在 hw3-1 中我主要平行化的地方有：
- 對 Dist 做初始化的時候
- 在 Floyd-Warshall Algorithm 中計算與比較 ij 與 ik、kj 距離最小值的時候。

### (1) 對 Dist 做初始化的時候

我使用 OpenMP 平行處理對 Dist 的初始化：

```cpp
// static int D[50010][50010];
// int n, m;

FILE* file = fopen(infile, "rb");

fread(&n, sizeof(int), 1, file);
fread(&m, sizeof(int), 1, file);

#pragma omp parallel for schedule(static)
for (int i = 0; i < n; ++i) {
    for (int j = 0; j < n; ++j) {
        if (i == j) {
            D[i][j] = 0;
        } else {
            D[i][j] = INF;
        }
    }
}

int* edges = (int*)malloc(m * 3 * sizeof(int));

fread(edges, sizeof(int), m * 3, file);

#pragma omp parallel for schedule(static)
for (int i = 0; i < m; ++i) {
    D[edges[i * 3]][edges[i * 3 + 1]] = edges[i * 3 + 2];
}
```

### (2) 計算與比較 ij 與 ik、kj 距離最小值的時候

我使用 OpenMP 平行處理 Blocked Floyd-Warshall Algorithm 中三層分別對 `b_i`、`b_j`、`k` 做的 `for` 迴圈。

```cpp
#pragma omp parallel for collapse(2) schedule(static)
for (int b_i = block_start_x; b_i < block_end_x; ++b_i) {
    for (int b_j = block_start_y; b_j < block_end_y; ++b_j) {
        for (int k = k_start; k < k_end; ++k) {
            // Calculation
        }
    }
}
```

由於 `b_i`、`b_j` 是當前 Block 的位置，內部還要有兩層分別對 Block 中元素 `i`、`j`，`for` 迴圈的平行化，這裡採用 SSE (Streaming SIMD Extensions) 指令集加速內部的計算與比較。設定一次做 4 個元素並且迴圈展開 4 次，一次處理 16 個元素。

```cpp
//const int SIMD_WIDTH = 4;
//const int UNROLL = 4;

for (int i = block_internal_start_x; i < block_internal_end_x; ++i) {
    __m128i v_d_ik = _mm_set1_epi32(D[i][k]); // 每個 j 使用相同的 D[i][k]
    
    int j;
    for (j = block_internal_start_y; j + SIMD_WIDTH * UNROLL <= block_internal_end_y; j += SIMD_WIDTH * UNROLL) {
        
        // 載入 4 組，每組 4 個 32-bit 整數
        __m128i v0_d_kj = _mm_loadu_si128((__m128i*)&D[k][j]);
        __m128i v1_d_kj = _mm_loadu_si128((__m128i*)&D[k][j + SIMD_WIDTH]);
        // ...

        // 載入當前的最短路徑值
        __m128i v0_d_ij = _mm_loadu_si128((__m128i*)&D[i][j]);
        // ...

        // 計算新路徑長度
        __m128i v0_sum = _mm_add_epi32(v_d_ik, v0_d_kj);
        // ...

        // 取最小值
        v0_sum = _mm_min_epi32(v0_sum, v0_d_ij);
        // ...

        // 更新結果
        _mm_storeu_si128((__m128i*)&D[i][j], v0_sum);
        // ...
    }

    // 處理剩餘元素
    for (; j < block_internal_end_y; ++j) {
        int d_ik_kj = D[i][k] + D[k][j];
        D[i][j] = std::min(D[i][j], d_ik_kj);
    }
}
```

## b. How do you divide your data in hw3-2, hw3-3?
### &
## c. What’s your configuration in hw3-2, hw3-3? And why?

### hw3-2

> e. Briefly describe your implementations in diagrams, figures or sentences.

首先在 hw3-2，Dist 不同於 hw3-1，hw3-1 採用固定大小的二維矩陣，hw3-2 採用動態大小的一維陣列，這是由於之後在搬運資料時可以使用 `cudaMemcpy` 簡單的搬運至 GPU Global Memory 並且在 Kernel 計算時可以減少 `if` 造成 Branching Divergence 的問題。

```cpp
int* h_D, d_D;

n = N + (FW_B - (N % FW_B)); // 將節點數量成為FW_B的倍數，可以讓Kernel中不使用if做邊界檢查

h_D = (int*)malloc(n * n * sizeof(int));

// ... init h_D same as in hw3-1.

cudaMalloc(&d_D, n * n * sizeof(int));

cudaMemcpy(d_D, h_D, n * n * sizeof(int), cudaMemcpyHostToDevice);
```

---

我在 hw3-2 的實作中，我定義 `FW_B` 與 `CUDA_B`，`FW_B` 是切分 Dist 數個由 `FW_B` * `FW_B` 組成的 Block，`CUDA_B` 是在 CUDA Kernel Launch 所定義的 Block Size 大小，例如 `dim3 block(CUDA_B, CUDA_B)`，也就是在"邏輯"層面上，一個 CUDA Kernel 總共有 `CUDA_B` * `CUDA_B` 個 threads 在處理資料。

:::info
**為什麼需要分為 `FW_B` 與 `CUDA_B` 呢？**

因為在我的實作中，一個 CUDA block 中的 thread 可能不只計算一個 `FW_B` * `FW_B` 中的元素，我將一個 thread 需要計算元素的數量定義為 `TILE`，一個 thread 需要負責 `TILE` * `TILE` 個元素的計算。
:::

在我的實作中我設定：
- `FW_B`：76
- `TILE`：4
- `CUDA_B`：FW_B / TILE（76 / 4 = 19）

大致切分方法如下圖所示（TILE = 2）：
![image](https://hackmd.io/_uploads/r123a8nVkg.png)
:::info
圖：將 Dist 切分為由數個 `FW_B` * `FW_B` 組成的 Block，每一個 Block 切成 `TILE` * `TILE` 的 Tile Block，每一個 Tile Block 中的數字代表 threadIdx 要負責的資料，總共有 `CUDA_B` * `CUDA_B` 個 threads 來處理。
:::

我採用 `FW_B` 與 `CUDA_B` 還有 `TILE` 是因為，在我一開始的實作中我發現，當每一個 CUDA thread 只負責處理 1 個資料的時候，由於任務過於簡單（只有加法與比較）thread 處理太過於快速，導致 overhead 會出現在 CUDA 管理以及建立這些 threads 上，代表一個 thread 可以負責處理更多的資料，推論是可以相對減輕對於管理 threads 的影響程度。所以將 thread 設定可以一次做 `TILE` * `TILE` 的計算，增加運行效率。

---

我在 hw3-2 的實作中，我將 Blocked Floyd-Warshall Algorithm 中 Phase1、2、3 分別寫成 3 個 CUDA Kernel Function，分別處理 Phase1、2、3 的計算。

其中 Phase2 又拆成 `phase2_row_kernel` 與 `phase2_col_kernel` 分別做 Pivot Row / Pivot Column Blocks 的計算，所以總共有 1 + 2 + 1 = 4 個 CUDA Kernel Function。

如同 hw3-1，首先我們計算所需處理的 `round` 數

```cpp
const int round = n / FW_B; // 不用取Ceiling，因為先前將n變成一定為FW_B的倍數
```

![image](https://hackmd.io/_uploads/Bk-qw_24kg.png)

#### - Phase1

`phase1_kernel` 中有一個 `__shared__ int s_D[FW_B][FW_B]` 的 Shared Memory 空間，每一個 thread 將自己負責的 Tile 座標上的資料從 Global Memory Load 至 Shared Memory 中，可以消除待會在做 Floyd-Warshall Algorithm 中對 Global Memory 多次的 Access。

```cpp
// load data form global memory to shared memory
for (int bi = 0; bi < TILE; bi++) {
    for (int bj = 0; bj < TILE; bj++) {
        // ... convert local index to global index

        s_D[i][j] = d_D[global_i * n + global_j];
    }
}

// do Floyd-Warshall algorithm
for (int k = 0; k < FW_B; k++) {
    for (int bi = 0; bi < TILE; bi++) {
        for (int bj = 0; bj < TILE; bj++) {
            // ... convert local index to global index

            s_D[i][j] = min(s_D[i][j], s_D[i][k] + s_D[k][j]);
        }
    }
}

// store data back to global memory
for (int bi = 0; bi < TILE; bi++) {
    for (int bj = 0; bj < TILE; bj++) {
        // ... convert local index to global index

        d_D[global_i * n + global_j] = s_D[i][j];
    }
}
```

`phase1_kernel` 的 Kernel Launch 在 grid 設定上只有 1，因為只有一個 Pivot Block。

```cpp
phase1_kernel<<<1, block, 0, stream1>>>(d_D, n, r);
cudaStreamSynchronize(stream1);
```

#### - Phase2

`phase2_row_kernel` 與 `phase2_col_kernel` 中各自有兩個 Shared Memory 空間，其中 `__shared__ int s_pivot[FW_B][FW_B]` 為用於儲存 Phase1 處理好的 Pivot Block。 在 Row Kernel 有 `__shared__ int s_row[FW_B][FW_B]` 用於儲存當前需要計算的 Block 資料；Column Kernel 則同樣有 `__shared__ int s_col[FW_B][FW_B]` 的 Shared Memory 空間。

同`phase2_col_kernel`，以`phase2_row_kernel` 為例：
```cpp
// load data form global memory to shared memory
for (int bi = 0; bi < TILE; bi++) {
    for (int bj = 0; bj < TILE; bj++) {
        // ... convert local index to global index

        s_pivot[i][j] = d_D[(start + i) * n + start + j];
        s_row[i][j] = d_D[global_i * n + global_j];
    }
}

// do Floyd-Warshall algorithm
for (int k = 0; k < FW_B; k++) {
    for (int bi = 0; bi < TILE; bi++) {
        for (int bj = 0; bj < TILE; bj++) {
            // ... convert local index to global index

            s_row[i][j] = min(s_row[i][j], s_row[i][k] + s_pivot[k][j]); // 利用 Pivot Block 比較並更新當前 Pivot Row
        }
    }
}

// store data back to global memory
for (int bi = 0; bi < TILE; bi++) {
    for (int bj = 0; bj < TILE; bj++) {
        // ... convert local index to global index

        d_D[global_i * n + global_j] = s_row[i][j];
    }
}
```

`phase2_col_kernel` 與 `phase2_row_kernel` 的 Kernel Launch 在 grid 設定上是 `round`，因為總共要做 `round` 個 FW_B（沒有排除 Pivot Block，可以減少在 Kernel Function 的 Branching Divergence）。

另外，`phase2_col_kernel` 與 `phase2_row_kernel` 我利用 CUDA Stream 平行處理。

```cpp
phase2_row_kernel<<<round, block, 0, stream2>>>(d_D, n, r);
phase2_col_kernel<<<round, block, 0, stream3>>>(d_D, n, r);

cudaStreamSynchronize(stream2);
cudaStreamSynchronize(stream3);
```

#### - Phase3

`phase3_kernel` 中有使用兩個 Shared Memory 空間，`__shared__ int s_row[FW_B][FW_B]`、`__shared__ int s_col[FW_B][FW_B]` 用於儲存 Phase2 處理好的 Pivot Row / Pivot Column Blocks，每一個 thread 自己使用 `register int result[TILE][TILE]` 用於儲存比較的結果（不使用__shared__是因為沒有空間再開一個 [FW_B][FW_B] 的 Shared Memory 了）。

```cpp
// load data form global memory to shared memory
for (int bi = 0; bi < TILE; bi++) {
    for (int bj = 0; bj < TILE; bj++) {
        // ... convert local index to global index

        result[bi][bj] = d_D[global_i * n + global_j];
        
        s_row[i][j] = d_D[global_i * n + start + j];
        s_col[i][j] = d_D[(start + i) * n + global_j];
    }
}

// do Floyd-Warshall algorithm
for (int k = 0; k < FW_B; k++) {
    for (int bi = 0; bi < TILE; bi++) {
        for (int bj = 0; bj < TILE; bj++) {
            // ... convert local index to global index

            result[bi][bj] = min(result[bi][bj], s_row[i][k] + s_col[k][j]); // 利用 Pivot Row / Pivot Column 比較並更新當前 Result
        }
    }
}

// store data back to global memory
for (int bi = 0; bi < TILE; bi++) {
    for (int bj = 0; bj < TILE; bj++) {
        // ... convert local index to global index

        d_D[global_i * n + global_j] = result[bi][bj];
    }
}
```

`phase3_kernel` 的 Kernel Launch 在 grid 設定上是 `round` * `round`，因為總共要做 `round` * `round` 個 FW_B （同理沒有排除 Pivot Block、Pivot Row / Pivot Column Blocks）

```cpp
phase3_kernel<<<grid, block, 0, stream1>>>(d_D, n, r);
cudaStreamSynchronize(stream1);
```

### hw3-3

在 hw3-3，採用與 hw3-2 相同的 Phase123 Kernel Function。

在我的實作中我設定：
- `FW_B`：75
- `TILE`：3
- `CUDA_B`：FW_B / TILE（75 / 3 = 25）

與 hw3-2 不同的是採用多張 GPU 的機制，首先兩張 GPU 各自進行 Phase 1 與 Phase 2 的計算，在 Phase 3 則是 GPU0 計算 : 前 `r` （當前`round`） 個 Columns 的 `FW_B`，GPU1 則是計算 `round` - `r` - 1 : 後 `remain_round` 個 Columns 的 `FW_B`。

以 r = 1 為例： GPU0 計算 : 前 1 個 Columns 的 `FW_B`，GPU1 則是計算 `round` - `r` - 1 : 後 1 個 Columns 的 `FW_B`，如下圖所示（黃色的部分由 GPU0 處理，綠色的部分由 GPU1 處理）：

![image](https://hackmd.io/_uploads/BJBY-F2Vyx.png)

在 `phase3_kernel` 新增 `offset`，讓 `global_i` 可以有不同的起始點。

```cpp
const int actual_y = blockIdx.y + offset;
```

```cpp
// int* d_D[GPU_NUM];


// init GPUs' global memory
for (int i = 0; i < GPU_NUM; i++) {
    cudaSetDevice(i);
    cudaDeviceEnablePeerAccess(1-i, 0);
    
    cudaMalloc(&d_D[i], n * n * sizeof(int));
    cudaMemcpy(d_D[i], h_D, n * n * sizeof(int), cudaMemcpyHostToDevice);
}

for (int r = 0; r < round; ++r) {
    const int remain_round = round - r - 1;
    
    const int offset = r * FW_B * n;

    #pragma omp parallel num_threads(GPU_NUM) // unsing OpenMP to control two GPUs independently
    {
        int i = omp_get_thread_num();

        cudaSetDevice(i);
        
        // Transfer data
        
        cudaDeviceSynchronize();
        #pragma omp barrier

        phase1_kernel<<<1, block, 0, streams[i][0]>>>(d_D[i], n, r);
        cudaStreamSynchronize(streams[i][0]);

        phase2_row_kernel<<<round, block, 0, streams[i][1]>>>(d_D[i], n, r);
        phase2_col_kernel<<<round, block, 0, streams[i][2]>>>(d_D[i], n, r);

        cudaStreamSynchronize(streams[i][1]);
        cudaStreamSynchronize(streams[i][2]);

        if (i == 0) {
            phase3_kernel<<<dim3(round, r), block, 0, streams[i][0]>>>(d_D[0], n, r, 0);
        } else {
            phase3_kernel<<<dim3(round, remain_round), block, 0, streams[i][0]>>>(d_D[1], n, r, r+1);
        }
    }
}
```

## d. How do you implement the communication in hw3-3?

在 hw3-3 由於 GPU1 只負責當前 `r` 後剩餘的 `remain_round` 個 Columns，隨著`r`迭代的次數增加，GPU1 負責的區域會逐漸縮小，而這些區域在下一輪會轉由 GPU0 負責處理。GPU1 必須將其處理完的結果傳送給 GPU0，才可以確保運算的正確性。

GPU0 用於整合輸出結果，因此 GPU0 需要擁有完整的運算結果才能將資料正確傳回 Host Memory。 GPU1 在扮演輔助運算的角色，GPU1 處理的結果最終都需要傳輸到 GPU0。

單向資料傳輸的設計可以減少不必要的記憶體傳輸開銷，也可以簡化結果收集的流程，同時確保整個運算過程的資料一致性。

```cpp
for (int r = 0; r < round; ++r) {
    const int remain_round = round - r - 1;
    
    const int offset = r * FW_B * n;
    
    #pragma omp parallel num_threads(GPU_NUM)
    {
        int i = omp_get_thread_num();

        cudaSetDevice(i);

        if (i == 1) {
            cudaMemcpyPeerAsync(d_D[0] + offset, 0, d_D[1] + offset, 1, FW_B * n * sizeof(int), streams[i][0]);
        }

        cudaDeviceSynchronize();
        #pragma omp barrier

        // do phase 123
    }
}
```

# 3. Profiling Results (hw3-2)

在 hw3-2 運行時長最久的 Kernel Function 是 `phase3_kernel`，因為 `phase3_kernel` 要處理的 Block 數最多。我使用 `c08.1` 與 `p11k1` 作為測量的 Testcase。

```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 nvprof --kernels "phase3_kernel" --metrics sm_efficiency,achieved_occupancy,shared_load_throughput,shared_store_throughput,gld_throughput,gst_throughput ./hw3-2 /home/pp24/share/hw3-2/testcases/c08.1 /share/judge_dir/a.out
```

![image](https://hackmd.io/_uploads/H1L7sYnNJg.png)

```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 nvprof --kernels "phase3_kernel" --metrics sm_efficiency,achieved_occupancy,shared_load_throughput,shared_store_throughput,gld_throughput,gst_throughput ./hw3-2 /home/pp24/share/hw3-2/testcases/p11k1 /share/judge_dir/a.out
```

![image](https://hackmd.io/_uploads/HyEKoF3EJe.png)

# 4. Experiment & Analysis

## a. System Spec

我的實驗都在課堂提供的 Apollo 與 Apollo GPU 中完成。hw3-2 我使用 `c08.1` 作為測量的 Testcase。

## b. Blocking Factor (hw3-2)

### 測量 `phase3_kernel`

- 測 Metrics：

```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 nvprof --kernels "phase3_kernel" --metrics inst_integer,gld_throughput,gst_throughput,shared_load_throughput,shared_store_throughput ./hw3-2 /home/pp24/share/hw3-2/testcases/c08.1 /share/judge_dir/a.out
```

- 測 Average Phase3 Kernel Time：
```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 nvprof ./hw3-2 /home/pp24/share/hw3-2/testcases/c08.1 /share/judge_dir/a.out --benchmark
```

測量不同 Blocking Factor 下各 Metrics 的差異，在我的實作中我固定 `TILE = 4`，並控制 FW_B 大小作為變因。

### 實驗結果

![image](https://hackmd.io/_uploads/r146NshNJx.png)

<div style="display: flex; justify-content: space-around; margin-top: 10px;">
<img src="https://hackmd.io/_uploads/ryb-Si3N1l.png" width="45%"> <img src="https://hackmd.io/_uploads/BkXZBshEJx.png" width="45%">
</div>

## c. Optimization (hw3-2)

我使用 `#include <chrono>` 的 `high_resolution_clock::now()` 作為測量時間的工具。

使用 `p20k1` 作為測量的 Testcase。

測量不同 Optimization 下的時間差異。在我的實作中我做 Tile 前使用 `FW_B = 32`，使用 Tile 後將 `Tile = 2`，並修改 `FW_B = 64`，所得到的 `CUDA_B = 32`。

- Padding 是將輸入的節點數 `n` 補齊為 `FW_B` 的倍數。
- Tile 將每個 thread 負責的計算資料量增加。
- Stream 將 `phase2_row_kernel` 與 `phase2_col_kernel` 平行處理。

最後做 Blocking Factor Tuning，將 `Tile = 4`，並修改 `FW_B = 76`，所得到的 `CUDA_B = 19`。

![image](https://hackmd.io/_uploads/rk3LLn2Vyl.png)

## d. Weak scalability (hw3-3)

![image](https://hackmd.io/_uploads/rJdDUJR41g.png)

![image](https://hackmd.io/_uploads/S1fmtkREJg.png)

hw3-3 在 `p20k1` 的 Speed Up 最顯著，但是到了 `p30k1` 就開始下降。由於 hw3-3 只有在 Phase3 進行資料的分配，且隨著 `round` 數越大 GPU1 負責的資料越少，再加上需要等待 GPU1 將 Phase3 運算完成的資料傳輸至 GPU0 的等待時間。因此整體而言，有諸多限制導致 Speed Up 相對緩慢。

## e. Time Distribution (hw3-2)

使用 `#include <chrono>` 的 `high_resolution_clock::now()` 與 Nsight System 作為測量時間的工具。

測量使用 `p11k1`、`p15k1`、`p20k1`、`p30k1` 作為 Testcase 的情況下，測量程式在 Compute Time、Total Memory Copy Time、Input Output Time 的時間組成。

```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 nsys profile --trace=cuda,osrt ./hw3-2 /home/pp24/share/hw3-2/testcases/p11k1 /share/judge_dir/a.out
```

![image](https://hackmd.io/_uploads/Sye3W10Vye.png)

# 5. Experiment on AMD GPU

## a. Nvidia GPU vs. AMD GPU

### SPEC

**[NVIDIA GeForce GTX 1080](https://www.techpowerup.com/gpu-specs/geforce-gtx-1080.c2839)**

- Launched by **May, 2016**
- 16 nm process
- GP104 graphics processor
- 8 GB GDDR5X memory
- Bandwidth 320.3 GB/s
- 256-bit memory interface
- FP32 (float) 8.873 TFLOPS

**[AMD Radeon Instinct MI210](https://www.techpowerup.com/gpu-specs/radeon-instinct-mi210.c3857)**

- Launched by **December, 2021**
- 6 nm process
- Aldebaran graphics processor
- 64 GB HBM2e memory
- Bandwidth 1.64 TB/s
- 4096-bit memory interface
- FP32 (float) 22.63 TFLOPS

---

暫存器容量：

- GTX 1080: 每個 SM 有64K 32位元暫存器
- MI210: 每個 CU 有128K 32位元暫存器

共享記憶體：

- GTX 1080: 每個 SM 有96KB
- MI210: 每個 CU 有128KB LDS (Local Data Share)

處理單元數量：

- GTX 1080: 20個 SM
- MI210: 104個 CU

:::info
AMD 的 GPU 中使用 Compute Unit (CU) 作為主要運算單元，類似 NVIDIA 的 Stream Multiprocessor (SM)。
:::

## b. Experiment

<div style="display: flex; justify-content: space-around; margin-top: 10px;">
<img src="https://hackmd.io/_uploads/HJ-UwxCV1g.png" width="45%"> <img src="https://hackmd.io/_uploads/S19UwgRNJl.png" width="45%">
</div>

<div style="display: flex; justify-content: space-around; margin-top: 10px;">
<img src="https://hackmd.io/_uploads/ryODwx0Eke.png" width="45%"> <img src="https://hackmd.io/_uploads/BysDPlAVye.png" width="45%">
</div>

## c. Insight & Reflection

### 性能差異：

在所有測試中，AMD MI210 比 NVIDIA GTX 1080 表現更好，尤其在較大的 Testcase 上差距更明顯 MI210在 `p30k1` 時約快1.7倍(單GPU)及1.6倍(雙GPU)

### 擴展性：

實際 Speed Up Factor 隨 Testcase 規模增加而提升，從 `p11k1` 的0.8倍到 `p30k1` 的1.1倍但與理想2倍速度提升仍有差距，顯示存在瓶頸。

### 效能瓶頸：

較小 Testcase (`p11k1` - `p15k1`)時，額外同步與通訊開銷抵消了平行運算優勢。

### 架構差異帶來的影響：

MI210 的更大記憶體頻寬 (1.64 TB/s vs 320.3 GB/s) 確實反映在大規模運算性能上4096位元記憶體介面 vs. 256位元的差異在`p30k1`等大規模問題特別明顯。

但硬體規格優勢並未完全轉化為等比例性能提升：理論運算能力差2.5倍(22.63 vs 8.873 TFLOPS)，實際測試僅達到1.7倍提升，記憶體頻寬差5倍，但效能未達同等提升。

### 啟發：

- 純硬體規格比較無法完全反映實際應用效能。
- 效能瓶頸可能來自於軟體優化、記憶體存取模式、任務調度等多個層面。
- 選擇硬體時需考慮實際應用場景，不能只看規格數字。

# 6. Experience & conclusion

作為初次接觸 CUDA 和 GPU 運算的初學者，這次的作業讓我深入理解了 CUDA 架構的複雜性和優化的重要性。Floyd-Warshall 演算法本身就具有高度的資料相依性，因此在 GPU 上的平行化實作特別具有挑戰性。在實作過程中，我學習到了許多重要的 GPU 程式設計概念，包括記憶體階層的管理、執行緒配置的優化，以及如何有效利用共享記憶體來減少全域記憶體的存取。

最具挑戰性的部分是雙 GPU 的實作。雖然我成功實現了基本的功能，但回顧整個過程，我發現在資料分配和 GPU 間通訊的設計上還有很大的改進空間。特別是在處理大規模資料集時，目前的實作方式可能無法充分發揮雙 GPU 的效能優勢。

這個作業也讓我體會到，CUDA 程式設計不僅需要對硬體架構有深入的理解，還需要考慮到許多細節，例如記憶體存取模式、執行緒配置、硬體限制等。雖然 CUDA 提供了強大的平行運算能力，但要有效地利用這些資源來加速如 Floyd-Warshall 這樣的演算法並不容易。這次的經驗讓我更加體會到在高效能運算領域中，理論知識與實作經驗同等重要，而且持續的優化和改進是達到最佳效能的關鍵。