# Homework 4

# 1. Title, name, student ID

Homework 4 Report, 張博皓, 112065530

# 2. Implementation

## a. Flash Attention Forward Pass

### (1) Input Specification

矩陣 `Q`、`K`、`V` 的大小分別為 $( B \times N \times d )$，其中：
- $B$ : Batch size
- $N$ : Sequence length
- $d$ : Feature dimension

![IMG_0102](https://hackmd.io/_uploads/rk8o5_tHyl.jpg)

### (2) Host Function

每個 GPU Thread 處理一個 Row 的資料，每個 Row 含有 $d$ 個元素，因此總共需要處理 $B \times N$ 個 Rows 的 Attention 計算。

在計算 Attention 時，我依據 SPEC 中 Flash Attention 演算法的步驟實現。

### (3) Kernel Function

在 GPU 的 HBM 中存放以下資料：
- `Q`、`K`、`V` : 輸入矩陣
- `O` : Attention 計算結果的輸出矩陣
- `L`、`M` : Attention 計算的中間結果

每個 Thread Block 共享以下變數的 Shared Memory：`kj`、`vj`、`qi`、`spij`，使用 Shared Memory 可以提高 threads 之間的資料重用率。

### (4) Pseudo Code

以下是我實作的主要邏輯虛擬碼：

```
function flash_attention_kernel(q, k, v, o, l, m, scalar, N, d, bc, br, tc, tr):

    1. Identify the current sequence row and batch being processed by this thread.

    2. Load the current row of Q into shared memory from HBM.

    3. Initialize:
           li = 0.0       # Normalization factor
           mi = -INFINITY # Maximum value

    for each chunk of K and V:
        4.  Load the chunk of K and V into shared memory from HBM.
        
        __syncthreads

        5.  Compute scaled dot products between Q and K for this chunk.
        
        6.  Update the maximum logit value (mi_tilde).

        7.  Apply the Softmax transformation:
                - Normalize logits using mi_tilde to avoid overflow.
                - Compute the new normalization factor (li_tilde).

        8.  Update global normalization factor (li_new) and max logit (mi_new).

        9.  Compute the weighted sum of V using the attention scores.
        
        10. Incrementally update the output O.

        11. Update the li and mi for this sequence row.
        
        __syncthreads
```

### (5) Shared Memory

共享記憶體存取方式示意圖：

![IMG_0104](https://hackmd.io/_uploads/rJ1n9OKHJe.jpg)

## b. Block Sizes and CUDA Kernel Configuration

### (1) Br 與 Bc 的選擇

由於每個 GPU Thread 負責一個 Row 的計算，且一個 Block 包含 `Br` 個 Rows，因此 `Br` 的大小直接影響程式的平行度。

GPU 的 Shared Memory 利用率越高，程式加速效益越好。然而 Shared Memory 資源有限，因此 `Bc` 與 `Br` 的關係受到 Shared Memory 大小的限制。計算所需的 Shared Memory 空間如下：
- `kj` : 大小為 $(Bc, d)$
- `vj` : 大小為 $(Bc, d)$
- `qi` : 大小為 $(Br, d)$
- `spij` : 大小為 $(Bc, Br)$

總共需要的 Shared Memory 大小為：

${Shared\ Memory\ Size} = \left(2 \times Bc \times d + Br \times d + Bc \times Br\right) \times \text{sizeof(float)}$

以 GPU 總共享記憶體大小 $M$ 為上限，已知 $d$ 與 $Br$，可計算出 $Bc$ 的最大值。

:::info
當 `Br` 設為 $2d$ 時，`Bc` 的計算公式為：

$Bc = \frac{M / 2 - d^2}{2d}$
:::

### (2) Kernel Configurations

Kernel 的配置如下：
1. 每個 Block 含有 `Br` 個 Threads 並行處理。
2. 總計 $( \frac{N}{Br} \times B )$ 個任務，對應 $B$ 個 Batch，每 Batch 需要 $( \frac{N}{Br} )$ 次 `Br` 的計算。

```cpp
dim3 grid((N + br - 1) / br, B); // 將 N 補齊至 br 的倍數，確保處理所有 N

dim3 block(br);

flash_attention_kernel<<<grid, block, shared_mem_size>>>(...);
```

# 3. Profiling Results

我使用 `t04` 與 `t22` 作為測量的 Testcase。

```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 --metrics achieved_occupancy,sm_efficiency,shared_load_throughput,shared_store_throughput,gld_throughput,gst_throughput ./hw4 /home/pp24/share/hw4/testcases/t04 /share/judge_dir/t.out
```

![image](https://hackmd.io/_uploads/ry2vMASSJe.png)

```bash
srun -pnvidia -N1 -n1 -c2 --gres=gpu:1 --metrics achieved_occupancy,sm_efficiency,shared_load_throughput,shared_store_throughput,gld_throughput,gst_throughput ./hw4 /home/pp24/share/hw4/testcases/t22 /share/judge_dir/t.out
```

![image](https://hackmd.io/_uploads/HyX6z0BS1g.png)

# 4. Experiment & Analysis

## a. System Spec

我的實驗都在課堂提供的 Apollo GPU 中完成。hw4 我使用 `t22` 作為測量的 Testcase。

## b. Optimization

我使用 `#include <chrono>` 的 `high_resolution_clock::now()` 作為測量時間的工具。

使用 `t22` 作為測量的 Testcase。

測量不同 Optimization 下的時間差異。

CPU 執行時間將近 282.391 s，然而 GPU Baseline 在只有對 Kernel 實作 Flash Attention 演算法且每個 Batch 獨立執行的情況下，執行時間超過 Apollo GPU srun 可執行時間限制，代表比 CPU 執行更久，推測是太多記憶體搬移以及 Kernel 效率低下導致的。

- 圖中 Batch 為將 Kernel Grid 調整成以 Batch 為任務分配，大幅加速執行時間。

- Adjust Algorithm 做了以下的調整：
  - (1) 資料讀取順序
  原先：外層對所有 Column （`j`）的迴圈，先將 `K` 和 `V` 的區塊加載到共享記憶體，再對行（`i`）進行操作。修改：外層對所有 Row （`i`）的迴圈，先對每行的 `Q` 加載到共享記憶體，再在內層處理所有列（`j`）的區塊。
  - (2) 減少 HBM 使用
  將 `L` 及 `M` 的中間結果，用 Register 儲存，減少對 HBM 的讀取。

- Finer Grid 將原本 `grid(B)` 改進為 `grid((N + br - 1) / br, B)` 提升任務分割的細粒度。

- Handle Bank Conflict 為 Shared Memory 加上 Padding 讓不同 Thread 存取 Bank 可以錯開。

- 設定 `Br = d`，以及最後計算 Shared Memory 可以支持最大的 `Bc` / `Br`。

![image](https://hackmd.io/_uploads/S1cpc1LSkg.png)

# 5. Experience & conclusion

HW4 與 HW3 雖然都是使用 CUDA 進行加速，但 HW4 主要的困難之處在於：如何利用 GPU 的平行處理能力計算 Attention。原本我以為這是一個簡單的題目，因為進行矩陣運算是 GPU 擅長的事情，但是我最初的實作，無法通過後面幾個較大的 Testcase，原因在於因為演算法的限制，反而無法開更多的 Thread 去進行處理。

Thread 間接受限於 Shared Memory 大小的限制，能夠改善效率的關鍵就在於如何在 Shared Memory 大小限制的條件下最大化 Threads 的執行效率。這件事情在優化程式上必須下點功夫，不能再無腦開 Threads 加速執行效率，因此優化的方向比較專注在任務顆粒度的分配、演算法實作的效率、處理 Bank Conflict、以及 Shared Memory 與 Threads 之間最大化效率的關係。透過這次的 HW4 讓我對於 CUDA 有更深入的了解。