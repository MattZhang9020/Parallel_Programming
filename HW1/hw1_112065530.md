# Homework 1

# 1. Title, name, student ID

Homework 1 Report, 張博皓, 112065530

# 2. Implementation

## 程式運行大綱

### i. 讀取資料

每個處理程序計算自己需要讀取的資料偏移量和數量：

> 各處理程序資料數量 = 資料總數 / 處理程序總數
> 偏移量 = 處理程序秩 * 各處理程序資料數量

(1). 使用 <code>MPI_File_open()</code> 打開輸入文件

(2). 使用 <code>MPI_File_read_at()</code> 函數，每個處理程序從計算得到的偏移位置開始，讀取屬於自己那部分的資料到 <code>local_data</code> 陣列

(3). 讀取完成後，使用 <code>MPI_File_close()</code> 關閉輸入文件

---

### ii. 處理程序內排序

使用順序的排序法（Sequential Sorting Method）將自己部分的資料進行排序（由小到大）

---

### iii. 奇偶排序

(1). 判斷當前階段（奇或偶）需要交換資料的同伴處理程序秩
(2). 與同伴處理程序交換排序好的資料
(3). 將同伴處理程序的資料與自己的資料進行合併排序（Merge Sort）
(4). 根據自身是左處理程序還是右處理程序，取合併排序結果的相應部分
(5). 將結果覆蓋至原本自身的資料
重複步驟(1)-(5)，直到完成<code>處理程序總數+1</code>次迭代

---

### iv. 輸出資料

(1). 使用 <code>MPI_File_open()</code> 打開輸出文件

(2). 使用 <code>MPI_File_write_at()</code> 函數，每個處理程序從計算得到的偏移位置開始，寫入自己排序好的資料

(3). 使用 <code>MPI_File_close()</code> 關閉輸出文件
釋放分配的記憶體並結束程序

## 程式運行細節

將輸入案例分為三種情況：

(1). 處理程序總數 = 資料總數

>例如：size = 4 (處理程序總數), n = 4 (資料總數)
>計算過程：
>- elements_per_process = n / size = 4 / 4 = 1
>- remainder = n % size = 4 % 4 = 0
>
>資料分配：
>- Rank 0: 1個元素 (local_n = 1)
>- Rank 1: 1個元素 (local_n = 1)
>- Rank 2: 1個元素 (local_n = 1)
>- Rank 3: 1個元素 (local_n = 1)

:::info
這種情況最簡單，只要兩兩互相交換資料，就可以進行比大小，將資料排序。
:::

---

(2). 處理程序總數 > 資料總數

>例如：size = 5 (處理程序總數), n = 3 (資料總數)
>計算過程：
>- elements_per_process = n / size = 3 / 5 = 0
>- remainder = n % size = 3 % 5 = 3
>
>資料分配：
>- Rank 0: 1個元素 (0 + 1，因為 rank < remainder)
>- Rank 1: 1個元素 (0 + 1，因為 rank < remainder)
>- Rank 2: 1個元素 (0 + 1，因為 rank < remainder)
>- Rank 3: 0個元素
>- Rank 4: 0個元素

:::warning
注意：在這種情況下，部分處理程序會分配到0個元素
:::

:::info
這種情況下，沒有資料的程序不要參與交換，所以要先確保程序自身與同伴程序皆具有資料才進行交換。
:::

---

(3). 處理程序總數 < 資料總數

>例如：size = 3 (處理程序總數), n = 10 (資料總數)

>計算過程：
>- elements_per_process = n / size = 10 / 3 = 3
>- remainder = n % size = 10 % 3 = 1
>
>資料分配：
>- Rank 0: 4個元素 (3 + 1，因為 rank < remainder)
>- Rank 1: 3個元素
>- Rank 2: 3個元素
>
>偏移量計算：
>- Rank 0: 0         (0 * 3 + 0)
>- Rank 1: 4         (1 * 3 + 1)
>- Rank 2: 7         (2 * 3 + 1)
>
:::info
這是最常見的狀況，幾乎所有程序都有多於1筆資料，程式所要做的是交換程序自身的資料與同伴程序的資料，程序將自己的資料與同伴的資料做合併排序，根據自身是小程序秩還是大程序秩來決定要取前半小的資料還是後半大的資料，如此完成一輪的排序。
:::

---

- 這個程式特別的地方是，有時候程序自身與同伴程序的資料數量不同，可以一開始就順便取得左右鄰居的資料數量，將來就不用傳遞這個數量資訊給同伴程序，減少溝通。

- 再來是程序自身在做合併排序時，只需程序自身原本資料數量的資料，所以可以提早結束排序，不必完全排序完成，且節省儲存空間。

- 使用指標交換合併排序的空間與程序自身儲存資料的空間，不必將合併排序完成的資料複製到儲存資料的空間，可以消除複製資料的時間。

- 使用指標直接存取陣列資料，並隨時移動指標，減少使用中括號存取陣列資料的時間。

- 程序根據自身程式秩的大小，一開始可以先與同伴程序交換最小或最大的資料，如此假設，同伴程序秩最小的資料比程序本身最大的資料大的話，可以確保程序本身的資料都小於同伴程序的資料，不必進行排序就不必再交換剩下的資料，減少溝通的成本。

- 在交換資料的時候可以先用二分搜尋法，先搜尋程序自身資料開始大於或小於的起點，將起點前或後的資料先放到合併排序完的空間，可以減少資料比較大小的時間成本。

# 3. Experiment & Analysis

## i. Methodology

### (a). System Spec

我的實驗都在課堂提供的 Appollo Cluster 中完成。

### (b). Performance Metrics

我使用<code>MPI_Wtime()</code>作為我的測量工具，<code>MPI_Wtime</code>會回傳過去任意時間點到當前的雙精度秒數，我可以透過時間相差來測量目標執行的所需時間。

我主要測量
<code>MPI_Init()</code>前到<code>MPI_Finalize()</code>作為<code>Total_time</code>

並且測量
<code>MPI_File_open()</code>、<code>MPI_File_read_at()</code>、
<code>MPI_File_write_at()</code>、<code>MPI_File_close()</code>
等MPI讀寫操作作為<code>IO_time</code>

最後測量
<code>MPI_Send()</code>、<code>MPI_Recv()</code>、
<code>MPI_Isend()</code>、<code>MPI_Irecv()</code>、
<code>MPI_Allgather()</code>、<code>MPI_Allreduce()</code>、<code>MPI_Wait()</code>
等MPI通訊操作作為<code>Comm_time</code>。

將<code>Total_time</code>扣除<code>IO_time</code>與<code>Comm_time</code>後，剩下的時間作為<code>CPU_time</code>。我使用課堂中提供的"33.txt"作為我實驗的測資，因為"33.txt"有足夠大量的資料，且需要多輪交換資料才能完成排序，足夠彰顯程式的性能表現。我將每次實驗做3次減少實驗中產生的隨機誤差。

## ii. Plots: Speedup Factor & Profile

### Experiment 1

>**使用單一節點（N=1），並且逐步增加 Process 數，觀察不同 Process 數下程式性能的表現。**

- 表現數據簡表

| Process Num | CPU Time | Communication Time | I/O Time | Total Time |
| -------- | -------- | -------- | -------- | -------- |
| 1 | 28.28 | 0.0 | 3.20 | 31.47 |
| 2 | 15.01 | 0.60 | 2.22 | 17.84 |
| ... | ... | ... | ... | ... |
| 12 | 4.17 | 2.40 | 1.83 | 8.41 |

![image0](https://hackmd.io/_uploads/SkCrT3weJg.png)
- 固定1個節點數，逐步增加 Process 數的時間分配圖。RUN TIME 單位為秒（s）

![image1](https://hackmd.io/_uploads/rkABTnPg1g.png)
- 固定1個節點數，逐步增加 Process 數的時間比例圖。RUN TIME 單位為秒（s）

![image2](https://hackmd.io/_uploads/rJRrahvgyx.png)
- 固定1個節點數，逐步增加 Process 數的 Speedup Factor，圖中藍色線為理想值、橘色線為實際值。

### Observation

根據上圖，首先可以觀察到 CPU 時間在單個 Process 的時候有最大的瓶頸，但逐漸增加 Process 數後， CPU 時間開始降低。我認為原因出在當單個 Process 的時候做排序，是使用線性排序，使用大量資料做線性排序非常耗 CPU 資源，但是當 Process 數多後，資料分配到各個 Process 上，每個 Process 以相對較小量的資料快速地完成初步的線性排序，較不花費 CPU 資源，因此 CPU 時間隨 Process 數下降。

利用平行 I/O 讀取與寫入，I/O 時間也因分配到的資料數目隨著 Process 數而逐漸減小。透過比例圖可以觀察到，隨著Process 數增多，通訊（橘色部分）時間逐漸增加，這是因為採用奇偶排序的方法，當 Process 數多的時候，需要來回做奇偶循環的次數變多，每一次都會產生溝通時間，當次數多時溝通時間就會不斷累積逐漸增長。

有趣的是 Process 數少溝通時間越少，我原本以為 Process 數多而傳輸的資料也多會造成明顯的溝通時間，但事實上同機器上做資料傳輸並不耗費太多時間。最後，這支程式在這個實驗的情況下相比線性處理（實驗中單個節點，1個 Process 的情況）有最多3.7倍的平行能力，原因是在各個 Process 做完初始的線性排序後，如在奇偶排序時，需要做溝通與合併排序的部分都無法平行處理，所以程式很難達到理想的平行化。

### Experiment 2

>**使用4個節點（N=4），並且逐步增加 Process 數，觀察不同 Process 數下程式性能的表現。**

- 表現數據簡表

| Process Num | CPU Time | Communication Time | I/O Time | Total Time |
| -------- | -------- | -------- | -------- | -------- |
| 4 | 8.19 | 1.60 | 1.0 | 10.80 |
| 12 | 4.21 | 1.94 | 1.54 | 7.69 |
| ... | ... | ... | ... | ... |
| 48 | 2.29 | 2.81 | 0.84 | 5.93 |

![image3](https://hackmd.io/_uploads/Hyeku6wxyl.png)
- 固定4個節點數，逐步增加Process數的時間分配圖。RUN TIME單位為秒（s）

![image4](https://hackmd.io/_uploads/rylyOpPlyl.png)
- 固定4個節點數，逐步增加Process數的時間比例圖。RUN TIME單位為秒（s）

![image5](https://hackmd.io/_uploads/rJxkd6PlJl.png)
- 固定4個節點數，逐步增加 Process 數的 Speedup Factor，圖中藍色線為實際值，實際值增長幅度相比於理想值差異較大，故忽略理想值。

### Observation

首先我第一件讓我意外的是，在同 Process 數（n=12）時，4節點的溝通時間（1.94s）小於1節點的溝通時間（2.40s），設備互相傳輸的瓶頸並沒有想像中那麼明顯。同樣的整體而言，隨 Process 數增加，程式用於溝通的時間就越多，但有趣的是到一個限度（如n=16）之後，溝通時間的增長幅度並沒有如之前大，我認為這是這份測資（33.txt）在不多的奇偶輪就可以完成排序的特性所致，在做奇偶排序的次數不變的情況下，效率增進的幅度因此就沒有增加。

### Experiment 3

>**觀察現有優化過的程式與過去初版舊的程式的性能表現差異。**

![image6](https://hackmd.io/_uploads/S1O4CpDl1x.png)
- 使用1個節點數，12個 Process，新版與舊版程式的性能表現。RUN TIME 單位為秒（s）

![image7](https://hackmd.io/_uploads/r1O40pPgJx.png)
- 使用4個節點數，12個 Process，新版與舊版程式的性能表現。RUN TIME 單位為秒（s）

### Observation

在舊程式中，一開始有使用到<code>MPI_Allgather()</code>用於蒐集 Process 的數量與每一輪奇偶排序使用<code>MPI_Allreduce()</code>用於檢查是否已經完成排序等，非常耗費溝通時間的方法。另外發現在初始線性排序在舊版採用 C++ Standard Library 中實作的<code>sort()</code>函數的排序效率不如新版中使用於 C++ boost library 中實作的<code>spreadsort()</code>方法，改用 spreadsort 可以增進初始線性排序的效率，但僅限於當所需排序的資料很多的時候，少量資料時幾乎沒有差別。整體而言，新版相比舊版有將近2.94倍的進步。

## iii. Discussion

> ### (1). Compare I/O, CPU, Network performance. Which is/are the bottleneck(s)? Why? How could it be improved?

從實驗結果來看，主要的效能瓶頸是隨著 Process 數增加而增長的通訊成本。在單一 Process 時，CPU 時間佔據大部分的執行時間（約90%），主要用於初始線性排序。然而，隨著 Process 數增加，CPU 時間顯著下降，但通訊時間卻相對增加。I/O 效能相對穩定且佔比較小，顯示平行 I/O 運作良好，不構成主要瓶頸。要改善這個系統，最關鍵是優化通訊方法。我認為可以使用非阻塞式通訊和調整資料塊大小或是資料串流等技術，也能有效降低通訊成本。通過這些優化，應該能進一步提升系統的整體效能和擴展性。

---

> ### (2). Compare scalability. Does your program scale well? Why or why not? How can you achieve better scalability?

從實驗數據可以觀察到程式的擴展性存在明顯的限制。在單節點(N1)配置下，逐漸增加 Process 數後，只達到了3.7倍的加速比，遠低於理想中的12倍；在四節點(N4)配置下，即使將處理程序擴展到48個，實際加速效果也趨於平緩。這種有限的擴展性主要受到兩個因素影響：首先是奇偶排序演算法本身的特性，需要多輪的同步和通訊，這部分無法被有效平行化；其次是隨著處理程序增加，程序間的通訊成本呈現上升趨勢，抵消了平行化帶來的效能提升。

如要改善擴展性，可以考慮採用更適合平行化的排序演算法，實作混合式架構或許是個可行方向，在節點內使用共享記憶體來減少通訊成本，節點間才使用訊息傳遞。通過減少同步區塊和優化通訊方法，應該能夠達到更好的擴展性表現。

# 4. Experiences / Conclusion

這份作業是我第一次接觸如何撰寫平行化程式，完成這份作業後我了解到，平行化程式的目標是如何最大地減少溝通成本，溝通的方式、資料的分配還有傳輸、如何在對的時機溝通以減少等待時間，都是平行化程式需要考量到的核心課題。我認為我在這份作業遇到的困難點，就是要如何以平行化的思維撰寫程式碼。平常以單線性的方式撰寫程式碼只需要考量到一個流程是否可以重頭到尾正常運行，但當平行程式不同，平行程式要考慮各個程序資料分配的問題、每一個程序該做什麼樣的事情，有時候並不是只有一個流程，而是同時有不同的流程在運行，但是最後都要一起同步結束，這種從線性到平行的撰寫思維轉換需要多加的訓練。